services:

  proxy-gateway:
    image: nginx:stable-alpine
    container_name: ai_proxy_gateway
    ports:
      - "8080:80"             # Port visible via Tailscale
    volumes:
      - ./proxy/nginx.conf.template:/etc/nginx/templates/nginx.conf.template:ro   # Mount template for envsubst
      - ./proxy/logs:/var/log/nginx                   # Mount log directory
    extra_hosts:
      - "host.docker.internal:host-gateway"  # Linux compatibility for host access
    environment:
      - OPENROUTER_API_KEY=${OPENROUTER_API_KEY}
      - GATEWAY_API_KEYS=${GATEWAY_API_KEYS}
      - NGINX_ENVSUBST_OUTPUT_DIR=/etc/nginx
      - LMSTUDIO_HOST=${LMSTUDIO_HOST:-${LAN_IP:-host.docker.internal}}
      - NGINX_ENVSUBST_FILTER=OPENROUTER_API_KEY|GATEWAY_API_KEYS|LMSTUDIO_HOST
    restart: unless-stopped



  faster-whisper:
    image: lscr.io/linuxserver/faster-whisper:latest   
    container_name: faster-whisper                             # remove if not using NVIDIA GPU
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=Europe/Istanbul
      - WHISPER_MODEL=base-int8
      - WHISPER_LANG=en
      - WHISPER_BEAM=1
    volumes:
      - ./faster-whisper-data:/config
    ports:
      - "10300:10300"
    restart: unless-stopped

  kokoro:
    image: ghcr.io/remsky/kokoro-fastapi-cpu:latest  # or kokoro-fastapi-gpu for CUDA
    container_name: kokoro
    environment:
      - TZ=Europe/Istanbul
    ports:
      - "8880:8880"
    restart: unless-stopped

  faster_whisper_rest:
    build: ./faster_whisper_rest
    image: aihub/faster-whisper-rest:latest
    container_name: faster_whisper_rest
    ports:
      - "10400:8000"
    volumes:
      - ./faster-whisper-data:/config   # if you want to reuse the model cache/config
    environment:
      - WHISPER_MODEL=small.en
      - DEVICE=cpu
      - COMPUTE_TYPE=int8
    restart: unless-stopped
  
