services:

  proxy-gateway:
    image: nginx:stable-alpine
    container_name: ai_proxy_gateway
    ports:
      - "8080:80"             # Port visible via Tailscale
    volumes:
      - ./proxy/nginx.conf.template:/etc/nginx/templates/nginx.conf.template:ro   # Mount template for envsubst
      - ./proxy/logs:/var/log/nginx                   # Mount log directory
    extra_hosts:
      - "host.docker.internal:host-gateway"  # Linux compatibility for host access
    environment:
      - OPENROUTER_API_KEY=${OPENROUTER_API_KEY}
      - NGINX_ENVSUBST_OUTPUT_DIR=/etc/nginx
      - LMSTUDIO_HOST=${LMSTUDIO_HOST:-${LAN_IP:-host.docker.internal}}
      - OLLAMA_HOST=${OLLAMA_HOST:-${LAN_IP:-host.docker.internal}}
    restart: unless-stopped

  openwebui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: openwebui
    ports:
      - "3000:8080"
    environment:
      - TZ=Europe/Istanbul
      - GLOBAL_LOG_LEVEL=info
    volumes:
      - ./openwebui-data:/app/backend/data
    depends_on:
      - faster-whisper
      - kokoro
    restart: unless-stopped

  faster-whisper:
    image: lscr.io/linuxserver/faster-whisper:latest   
    container_name: faster-whisper                             # remove if not using NVIDIA GPU
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=Europe/Istanbul
      - WHISPER_MODEL=base-int8
      - WHISPER_LANG=en
      - WHISPER_BEAM=1
    volumes:
      - ./faster-whisper-data:/config
    ports:
      - "10300:10300"
    restart: unless-stopped

  kokoro:
    image: ghcr.io/remsky/kokoro-fastapi-cpu:latest  # or kokoro-fastapi-gpu for CUDA
    container_name: kokoro
    environment:
      - TZ=Europe/Istanbul
    ports:
      - "8880:8880"
    restart: unless-stopped

  faster_whisper_rest:
    build: ./faster_whisper_rest
    image: aihub/faster-whisper-rest:latest
    container_name: faster_whisper_rest
    ports:
      - "10400:8000"
    volumes:
      - ./faster-whisper-data:/config   # if you want to reuse the model cache/config
    environment:
      - WHISPER_MODEL=small.en
      - DEVICE=cpu
      - COMPUTE_TYPE=int8
    restart: unless-stopped
  
  aihub_dashboard:
    build: ./dashboard
    container_name: aihub_dashboard
    ports:
      - "8090:8090"
    volumes:
      - ./proxy/logs:/var/log/nginx:ro
      - ./dashboard/data:/app/data
    environment:
      - AIHUB_IP=${AIHUB_IP:-${LAN_IP}}
      - LAN_IP=${LAN_IP}
      - DASHBOARD_API_KEYS=${DASHBOARD_API_KEYS}
      - OPENROUTER_API_KEY=${OPENROUTER_API_KEY}
      - OPENROUTER_MODEL=${OPENROUTER_MODEL}
    restart: unless-stopped
    depends_on:
      - kokoro
      - faster_whisper_rest
